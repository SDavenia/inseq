{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601ddf54-204e-4e83-a157-184baebdf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel, AutoModelForVision2Seq\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e412f887-f691-4884-a3ed-b5b0d00654a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/paligemma-3b-mix-224\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5612f6bd-7829-4b9d-ab63-58e603d14c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcd3b2963ba406e932be46339714482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForVision2Seq.from_pretrained(model_id, token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bb0dd1-9621-4bc1-8faf-8e569928b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "prompt = 'Describe this image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43fe4400-dca1-41f9-bc1f-d5808ba6018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logits_vector(inputs):\n",
    "    # Send to device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    vision_output = model.vision_tower(inputs['pixel_values'])\n",
    "    multimodal_projector_output = model.multi_modal_projector(vision_output['last_hidden_state'])\n",
    "    text_embeddings = model.get_input_embeddings()(inputs['input_ids']) # These contain <image> + prompt embeddings.\n",
    "    merge_output = model._merge_input_ids_with_image_features(multimodal_projector_output, text_embeddings, input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask'], labels = None, token_type_ids=None, cache_position=None)\n",
    "\n",
    "    inputs_embeds, attention_mask, labels, position_ids = merge_output\n",
    "    print(f\"input_embeds[0, 5, :10] shape: {inputs_embeds.shape}\\n\\t{inputs_embeds[0, 5, :10]}\")\n",
    "    outputs = model.language_model(attention_mask=attention_mask, \n",
    "                                   position_ids=position_ids, \n",
    "                                   inputs_embeds=inputs_embeds)\n",
    "    logits = outputs.logits\n",
    "    last_logit = logits[0, -1, :]\n",
    "\n",
    "    return last_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028fc895-3df4-453f-af91-630db3589e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embeds[0, 5, :10] shape: torch.Size([1, 261, 2048])\n",
      "\ttensor([ 0.0268, -0.0039, -0.0020, -0.0092,  0.0053, -0.0089, -0.0032,  0.0156,\n",
      "        -0.0121,  0.0059], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('inseq/extra_samu/data/image_test.png')\n",
    "img = img.convert('RGB')\n",
    "image_inputs = processor(prompt, img)\n",
    "last_logit_image = extract_logits_vector(image_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c9d86-f43c-4606-9e15-09ef941e1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.encode('Describe this image\\ncongruent angles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869214e-e48b-4fcd-a093-8fccf7cb4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.encode('Describe this image congruent angles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09531576-2f48-463f-bd6f-c011bec2c40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embeds[0, 5, :10] shape: torch.Size([1, 261, 2048])\n",
      "\ttensor([ 0.0472,  0.0010,  0.0035, -0.0238,  0.0039, -0.0368, -0.0042,  0.0173,\n",
      "        -0.0120,  0.0135], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "black_img = Image.new('RGB', (100, 100))\n",
    "black_inputs = processor(prompt, black_img)\n",
    "last_logit_black = extract_logits_vector(black_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa41b06b-cf96-4dcf-8421-5d3228c70ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: con\n",
      "logits: tensor([  0.4453,   5.9641, -11.5594,  ...,   0.3837,   0.3817,   0.3834],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Prediction: un\n",
      "logits: tensor([  0.2063,   3.2383, -13.9596,  ...,   0.1951,   0.1927,   0.1930],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction: {processor.decode(torch.argmax(last_logit_image))}\\nlogits: {last_logit_image}\\n\\n\")\n",
    "print(f\"Prediction: {processor.decode(torch.argmax(last_logit_black))}\\nlogits: {last_logit_black}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf04595-20c0-40a3-95f7-09039ea10297",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_probs = torch.nn.functional.softmax(last_logit_image, dim = 0)\n",
    "black_probs = torch.nn.functional.softmax(last_logit_black, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8dd8d8-571b-4d67-8ba9-4230b62bc0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl divergence should be either: 4.857807636260986 or 3.772852897644043\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "kl_i_b = entropy(image_probs.detach().numpy(), black_probs.detach().numpy())\n",
    "kl_b_i = entropy(black_probs.detach().numpy(), image_probs.detach().numpy())\n",
    "print(f\"kl divergence should be either: {kl_i_b} or {kl_b_i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba96120-171a-4d83-9f1e-a0873f4163ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Investigate the different tokenization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9473b73-632a-414f-abd1-82a6fb17fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have decoded:\n",
      "\t-con-\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have decoded:\\n\\t-{processor.decode(torch.argmax(last_logit_image), skip_special_tokens=False)}-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1bdbaff-d9c8-40b5-a04d-cbe226c70fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152,      2,  50721,    736,   2416,    108]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da1cbddb-c701-4bb0-883c-dcd57c3e55e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(108) # \\n is added automatically at the end, so it is correct that the first generation \n",
    "                      #   does not contain a beginning space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e574d07-1b3d-47c7-a336-366acb3663a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.2995], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_logit_image[processor.tokenizer.encode('con')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc874c68-7e9e-409c-8f6c-7eedebee624d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0420], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_logit_image[processor.tokenizer.encode(' con')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1afbf948-050e-4015-ba6e-ee9a450429d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[759, 16780, 579]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.encode('congruent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd5b09-de2c-44d7-8ffb-7f447926a5d0",
   "metadata": {},
   "source": [
    "**Finish generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0deea3ae-7ecc-4762-b036-5a65bde32062",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_generation = model.generate(**image_inputs, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6633ca91-8043-4164-a0e8-47330cd04672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_generation:\n",
      "\ttorch.Size([1, 261])\n",
      "Shape of output_generation:\n",
      "\ttorch.Size([1, 267])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of input_generation:\\n\\t{image_inputs['input_ids'].shape}\")\n",
    "print(f\"Shape of output_generation:\\n\\t{output_generation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7a54a96-09fe-4f6f-8f0c-5b9d9c9562e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['con-', 'gru-', 'ent-', ' angles-', ' .-', '<eos>-']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[processor.decode(x) + '-' for x in output_generation[0]][image_inputs['input_ids'].shape[1]:] # 6 generated tokens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6af399-0252-4517-a18a-77a53fb33df9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Compare with the logits obtained via inseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3d32d8-9679-4cf5-ad63-c7bd1bba90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information for the generation with the image \n",
    "img = Image.open('inseq/extra_samu/data/image_test.png')\n",
    "img = img.convert('RGB')\n",
    "image_inputs = processor(prompt, img)\n",
    "vision_output = model.vision_tower(image_inputs['pixel_values'])\n",
    "multimodal_projector_output = model.multi_modal_projector(vision_output['last_hidden_state'])\n",
    "text_embeddings = model.get_input_embeddings()(image_inputs['input_ids']) # These contain <image> + prompt embeddings.\n",
    "merge_output = model._merge_input_ids_with_image_features(multimodal_projector_output, text_embeddings, input_ids = image_inputs['input_ids'], attention_mask = image_inputs['attention_mask'], labels = None, token_type_ids=None, cache_position=None)\n",
    "\n",
    "image_inputs_embeds, attention_mask, labels, position_ids = merge_output\n",
    "#print(f\"input_embeds[0, 5, :10] shape: {image_inputs_embeds.shape}\\n\\t{image_inputs_embeds[0, 5, :10]}\")\n",
    "outputs_image = model.language_model(attention_mask=attention_mask, \n",
    "                               position_ids=position_ids, \n",
    "                               inputs_embeds=image_inputs_embeds)\n",
    "logits_image = outputs_image.logits\n",
    "last_logit_image = logits_image[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4760363-a66b-4a24-9c0c-a992b3fd1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Extract information for the generation with the black \n",
    "black_img = Image.new('RGB', (100, 100))\n",
    "black_inputs = processor(prompt, black_img)\n",
    "vision_output = model.vision_tower(black_inputs['pixel_values'])\n",
    "multimodal_projector_output = model.multi_modal_projector(vision_output['last_hidden_state'])\n",
    "text_embeddings = model.get_input_embeddings()(black_inputs['input_ids']) # These contain <image> + prompt embeddings.\n",
    "merge_output = model._merge_input_ids_with_image_features(multimodal_projector_output, text_embeddings, input_ids = black_inputs['input_ids'], attention_mask = black_inputs['attention_mask'], labels = None, token_type_ids=None, cache_position=None)\n",
    "\n",
    "black_inputs_embeds, attention_mask, labels, position_ids = merge_output\n",
    "#print(f\"input_embeds[0, 5, :10] shape: {image_inputs_embeds.shape}\\n\\t{image_inputs_embeds[0, 5, :10]}\")\n",
    "outputs_black = model.language_model(attention_mask=attention_mask, \n",
    "                               position_ids=position_ids, \n",
    "                               inputs_embeds=black_inputs_embeds)\n",
    "logits_black = outputs_black.logits\n",
    "last_logit_black = logits_black[0, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ab65e-8ce3-4930-923a-967238a863ad",
   "metadata": {},
   "source": [
    "- - - \n",
    "Extract inseq logits and compare (didn't resave them so ignore for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f711bdc-0329-489b-95d6-35ed65e9c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "batch_logits = torch.from_numpy(np.loadtxt('inseq/original_logits.txt')) # Black\n",
    "contrast_batch_logits = torch.from_numpy(np.loadtxt('inseq/contrast_logits.txt')) # Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c35ce04e-cafc-4901-a674-f0b68ad5417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([257216])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_logits.shape # I Saved the last ones that's why it results wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd806b34-e071-4e56-be31-3e9df9dce2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode([torch.argmax(batch_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c813f1-6aac-4f56-b1d6-70265a263b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode([torch.argmax(contrast_batch_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "969fea7f-e2ea-4e6f-bd0c-bd281bd0b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From inseq logits for black image are: tensor([-1.7957, 13.0201, -9.0444, -1.8154, -2.3267], dtype=torch.float64)\n",
      "From here logits for black image are:  tensor([  0.2063,   3.2383, -13.9596,   0.1729,  -0.0149],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"From inseq logits for black image are: {batch_logits[0:5]}\")\n",
    "print(f\"From here logits for black image are:  {last_logit_black[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67019a0a-f366-4aae-8057-5d517fa3c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From inseq logits for image are: tensor([-1.3816, 13.3314, -9.0542, -1.4042, -1.7697], dtype=torch.float64)\n",
      "From here logits for image are:  tensor([  0.4453,   5.9641, -11.5594,   0.4139,   0.9109],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"From inseq logits for image are: {contrast_batch_logits[0:5]}\")\n",
    "print(f\"From here logits for image are:  {last_logit_image[0:5]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0b054-8195-4bb5-a7c7-08b970fc9800",
   "metadata": {},
   "source": [
    "- - - \n",
    "**Check the embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5be736-aab1-4150-8070-78d7557dbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_embeddings = torch.from_numpy(np.loadtxt('inseq/original_inputs_embeddings.txt')) # Black\n",
    "contrast_batch_embeddings = torch.from_numpy(np.loadtxt('inseq/contrast_inputs_embeddings.txt')) # Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a719b89-2af4-4e20-95b1-6b56bab533b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([261, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e39e0bb-aaf4-4788-a27b-e3699d51ee58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0472,  0.0010,  0.0035, -0.0238,  0.0039, -0.0368, -0.0042,  0.0173,\n",
       "        -0.0120,  0.0135], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_embeddings[5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c660d8be-9b6c-47e7-b770-5a71b1034412",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_inputs_embeds = black_inputs_embeds.squeeze(0)\n",
    "image_inputs_embeds = image_inputs_embeds.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e57032e-d9e9-4321-835d-dbe25c0017a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all((black_inputs_embeds == batch_embeddings).flatten()) # Black embeddings from here are same as those in inseq!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ea3f4e1-535c-4448-8346-c00119397eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all((image_inputs_embeds == contrast_batch_embeddings).flatten()) # Black embeddings from here are same as those in inseq!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33bca021-e400-40ff-912a-ddc522f61e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with inseq embeddings\n",
    "outputs = model.language_model(attention_mask=attention_mask[:, :, :261, :261], \n",
    "                               position_ids=position_ids[:, :261],\n",
    "                               inputs_embeds=batch_embeddings.unsqueeze(0).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffce0cb7-149e-4145-99ea-d116f70abd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(torch.argmax(outputs.logits[0, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f25f3f00-41b2-4582-b527-d03022cb7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with inseq embeddings\n",
    "outputs = model.language_model(attention_mask=attention_mask[:, :, :261, :261], \n",
    "                               position_ids=position_ids[:, :261],\n",
    "                               inputs_embeds=contrast_batch_embeddings.unsqueeze(0).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "129c62d6-b402-45cd-b23b-99a2425605fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'con'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(torch.argmax(outputs.logits[0, -1, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8f8aa-39b4-44c5-a36e-89ca0a83980e",
   "metadata": {},
   "source": [
    "### Repeat for all steps steps:\n",
    "CHECK HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4bf6c9-935b-4963-8478-e5f1d099b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = processor.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c70fb27-9328-4ae1-8072-329af86a1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embeds[0, 5, :10] shape: torch.Size([1, 261, 2048])\n",
      "\ttensor([ 0.0268, -0.0039, -0.0020, -0.0092,  0.0053, -0.0089, -0.0032,  0.0156,\n",
      "        -0.0121,  0.0059], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# For next step\n",
    "prompt = 'Describe this image'\n",
    "img = Image.open('inseq/extra_samu/data/image_test.png')\n",
    "img = img.convert('RGB')\n",
    "image_inputs = processor(prompt, img)\n",
    "last_logit_image = extract_logits_vector(image_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8793fba-9a34-4453-ad77-06a62153d4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "processor.decode(torch.argmax(last_logit_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf482c-b05e-4647-ae6c-a19ee09d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_img = Image.new('RGB', (100, 100))\n",
    "black_inputs = processor(prompt, black_img)\n",
    "last_logit_black = extract_logits_vector(black_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
